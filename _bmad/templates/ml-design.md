# ML/RL Design: {Feature Name}

**Date**: {YYYY-MM-DD}
**Author**: Yoni + Alexios + Dulcy
**Status**: Draft | In Review | Approved

---

## 1. Problem Definition

### 1.1 Learning Objective
{What should the model learn to do?}

### 1.2 Input Space
| Input | Type | Shape | Description |
|-------|------|-------|-------------|
| Game state | Observation | (N,) | {Description} |
| Legal actions | Mask | (M,) | {Description} |

### 1.3 Output Space
| Output | Type | Shape | Description |
|--------|------|-------|-------------|
| Action | Discrete | (M,) | {Description} |
| Value | Continuous | (1,) | {Description} |

### 1.4 Constraints
- {Constraint 1}
- {Constraint 2}

---

## 2. Architecture

### 2.1 Model Type
{PPO, DQN, A2C, etc.}

### 2.2 Network Architecture
```
Input (N,)
    │
    ▼
Linear(N → 256) + ReLU
    │
    ▼
Linear(256 → 256) + ReLU
    │
    ├─────────────────────┐
    ▼                     ▼
Policy Head          Value Head
Linear(256 → M)      Linear(256 → 1)
    │                     │
    ▼                     ▼
Action Probs         State Value
```

### 2.3 Gymnasium Environment
```python
class CartesSocieteEnv(gym.Env):
    observation_space = gym.spaces.Box(...)
    action_space = gym.spaces.Discrete(...)
```

---

## 3. Training Plan

### 3.1 Curriculum
| Phase | Steps | Opponent | Notes |
|-------|-------|----------|-------|
| 1 | 0-50k | RandomPlayer | Learn basics |
| 2 | 50k-100k | HeuristicPlayer | Learn strategy |
| 3 | 100k+ | Self-play | Refine |

### 3.2 Hyperparameters
| Parameter | Value | Rationale |
|-----------|-------|-----------|
| learning_rate | 3e-4 | Standard for PPO |
| gamma | 0.99 | Long-horizon game |
| clip_epsilon | 0.2 | Standard PPO |
| n_steps | 2048 | Sufficient trajectory |
| batch_size | 64 | Memory efficient |

### 3.3 Reward Shaping
```python
def compute_reward(prev_state, action, new_state):
    reward = 0.0

    # Terminal rewards
    if new_state.is_game_over():
        return 1.0 if won else -1.0

    # Shaping rewards
    reward += 0.15 if action.is_evolution else 0.0
    reward += 0.01 * health_differential_change
    reward -= 0.001  # Time penalty

    return reward
```

---

## 4. Evaluation

### 4.1 Metrics
| Metric | Target | Measurement |
|--------|--------|-------------|
| Win rate vs Random | >80% | 1000 games |
| Win rate vs Heuristic | >60% | 1000 games |
| Training stability | Low variance | TensorBoard |

### 4.2 Validation Strategy
- Cross-validation against multiple opponents
- Holdout opponent set not seen during training
- Ablation studies for reward shaping

---

## 5. Infrastructure

### 5.1 Training Script
```bash
uv run python scripts/train_ppo.py \
    --config configs/training/ppo_default.yaml \
    --output models/ppo_v1
```

### 5.2 Monitoring
- TensorBoard for training curves
- Periodic evaluation games logged
- Checkpoint every 10k steps

### 5.3 Resource Requirements
- CPU: 8 cores (parallel environments)
- RAM: 8GB
- GPU: Optional
- Time: ~24 hours for full curriculum

---

## 6. Risks and Mitigations

| Risk | Probability | Mitigation |
|------|-------------|------------|
| Reward hacking | Medium | Careful reward design, manual inspection |
| Overfitting to opponent | Medium | Curriculum + self-play |
| Training instability | Low | PPO is stable; tune if needed |

---

## 7. Open Questions

- [ ] {Question 1}
- [ ] {Question 2}

---

*Generated by BMAD+AGENTIC Planning Framework (ML_DESIGN workflow)*
