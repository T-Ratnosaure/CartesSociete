# YOLOv8 Training Configuration for Card Detection
# ================================================
#
# This configuration file defines hyperparameters for training a YOLOv8
# model to detect 159 unique playing cards from CartesSociete.
#
# Usage:
#   uv run python scripts/train_card_detector.py --config configs/yolo_training.yaml
#
# Reference: https://docs.ultralytics.com/modes/train/

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
model:
  # YOLOv8 variant: n (nano), s (small), m (medium), l (large), x (extra-large)
  # Recommendation: 's' for balanced accuracy/speed, 'n' for mobile-first
  variant: "s"

  # Pretrained weights (from Ultralytics or custom path)
  pretrained_weights: "yolov8s.pt"

  # Number of card classes (fixed by repository)
  num_classes: 159

  # Input image size (must match data generation)
  image_size: 640

# -----------------------------------------------------------------------------
# Training Hyperparameters
# -----------------------------------------------------------------------------
training:
  # Maximum training epochs
  epochs: 100

  # Training batch size (adjust based on GPU memory)
  # - RTX 3090/4090: 32-64
  # - RTX 3080: 16-32
  # - RTX 3070: 8-16
  batch_size: 16

  # Early stopping patience (epochs without improvement)
  patience: 20

  # Optimizer: SGD, Adam, AdamW
  optimizer: "AdamW"

  # Learning rate schedule
  learning_rate: 0.001        # Initial learning rate
  learning_rate_final: 0.01   # Final LR factor (lr * lrf)

  # SGD momentum / Adam beta1
  momentum: 0.937

  # L2 regularization
  weight_decay: 0.0005

  # Warmup settings
  warmup_epochs: 3.0
  warmup_momentum: 0.8
  warmup_bias_lr: 0.1

  # Hardware settings
  device: ""  # Auto-detect (cuda:0, cpu, etc.)
  workers: 8  # DataLoader workers

  # Mixed precision training (faster on modern GPUs)
  amp: true

  # Image caching: false, "ram", or "disk"
  cache: false

  # Resume from checkpoint
  resume: false

  # Freeze backbone for initial epochs (transfer learning)
  # Set to 0 to train full model from start
  freeze_backbone_epochs: 10

# -----------------------------------------------------------------------------
# Training-Time Augmentation
# -----------------------------------------------------------------------------
# Note: Heavy augmentations are applied in Phase 1 synthetic data generation.
# These are lighter augmentations applied during training for additional variety.
augmentation:
  # Color space augmentation
  hsv_h: 0.015  # Hue shift (fraction)
  hsv_s: 0.7    # Saturation shift (fraction)
  hsv_v: 0.4    # Value shift (fraction)

  # Geometric augmentation
  # Note: Heavy rotation/scale already applied in Phase 1
  degrees: 0.0      # Rotation degrees
  translate: 0.1    # Translation fraction
  scale: 0.5        # Scale gain
  shear: 0.0        # Shear degrees
  perspective: 0.0  # Perspective transform

  # Flip augmentation
  # IMPORTANT: Cards should NOT be flipped
  flipud: 0.0  # Vertical flip probability
  fliplr: 0.0  # Horizontal flip probability

  # Composite augmentation
  mosaic: 0.5      # Mosaic probability (4-image composite)
  mixup: 0.1       # MixUp probability
  copy_paste: 0.0  # Copy-paste probability

# -----------------------------------------------------------------------------
# Export Configuration (ONNX)
# -----------------------------------------------------------------------------
export:
  # Export format: onnx, torchscript, tflite, coreml
  format: "onnx"

  # Export image size
  image_size: 640

  # ONNX opset version
  opset: 17

  # Simplify ONNX graph
  simplify: true

  # Dynamic input shapes (variable batch size)
  dynamic: true

  # Precision
  half: false   # FP16
  int8: false   # INT8 quantization

  # Include NMS in model
  nms: false

  # Export batch size
  batch_size: 1

# -----------------------------------------------------------------------------
# Inference Configuration
# -----------------------------------------------------------------------------
inference:
  # Minimum confidence for detections
  confidence_threshold: 0.25

  # IoU threshold for NMS
  iou_threshold: 0.45

  # Maximum detections per image
  max_detections: 300

  # Inference device
  device: ""  # Auto-detect

  # Use FP16 inference
  half: false

  # Verbose output
  verbose: false

# -----------------------------------------------------------------------------
# Paths and Logging
# -----------------------------------------------------------------------------
paths:
  # Path to data.yaml (generated by Phase 1)
  data_yaml: "data/training/data.yaml"

  # Output directory for training runs
  output_dir: "runs/detect"

  # Project name for logging
  project_name: "card_detector"

  # Experiment name (auto-generated if empty)
  experiment_name: ""

# -----------------------------------------------------------------------------
# Validation Settings
# -----------------------------------------------------------------------------
validation:
  # Validation frequency (epochs)
  val_interval: 1

  # Save best model
  save_best: true

  # Save periodic checkpoints
  save_period: 10

  # Generate plots
  plots: true

# -----------------------------------------------------------------------------
# Mobile-Optimized Preset
# -----------------------------------------------------------------------------
# To use mobile-optimized settings, override with:
#   --model.variant n
#   --export.int8 true
#   --export.dynamic false
mobile_preset:
  model:
    variant: "n"
    pretrained_weights: "yolov8n.pt"
  export:
    int8: true
    dynamic: false

# -----------------------------------------------------------------------------
# Accuracy-Focused Preset
# -----------------------------------------------------------------------------
# To use accuracy-focused settings, override with:
#   --model.variant m
#   --training.epochs 150
#   --training.batch_size 8
accuracy_preset:
  model:
    variant: "m"
    pretrained_weights: "yolov8m.pt"
  training:
    epochs: 150
    batch_size: 8
    patience: 30
    learning_rate: 0.0005
