{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Comparison\n",
    "\n",
    "This notebook compares different AI agent implementations, analyzing their performance characteristics and decision-making patterns.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from src.players import (\n",
    "    RandomPlayer,\n",
    "    GreedyPlayer,\n",
    "    HeuristicPlayer,\n",
    "    MCTSPlayer,\n",
    ")\n",
    "from src.players.mcts_player import MCTSConfig\n",
    "from src.simulation import GameRunner, MatchConfig, MatchRunner\n",
    "from src.analysis import BalanceAnalyzer, BalanceConfig, create_matchup_matrix\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Overview\n",
    "\n",
    "| Agent | Strategy | Complexity | Speed |\n",
    "|-------|----------|------------|-------|\n",
    "| Random | Random actions | O(1) | Very Fast |\n",
    "| Greedy | Best immediate value | O(n) | Fast |\n",
    "| Heuristic | Synergy + Evolution tracking | O(n) | Fast |\n",
    "| MCTS | Monte Carlo Tree Search | O(simulations * depth) | Slow |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Game Demonstration\n",
    "\n",
    "Run a single game to see the agents in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create players\n",
    "players = [\n",
    "    HeuristicPlayer(player_id=0, name=\"Heuristic\"),\n",
    "    GreedyPlayer(player_id=1, name=\"Greedy\"),\n",
    "]\n",
    "\n",
    "# Run a single game\n",
    "runner = GameRunner(players=players, seed=42, log_events=True, max_turns=30)\n",
    "result = runner.run_game()\n",
    "\n",
    "print(f\"Game completed in {result.turns} turns\")\n",
    "if result.winner_id is not None:\n",
    "    print(f\"Winner: {players[result.winner_id].name} ({result.winner_type})\")\n",
    "else:\n",
    "    print(\"Result: Draw\")\n",
    "\n",
    "print(f\"\\nPlayer Statistics:\")\n",
    "for pid, stats in result.player_stats.items():\n",
    "    print(f\"  {players[pid].name}:\")\n",
    "    print(f\"    Cards bought: {stats.cards_bought}\")\n",
    "    print(f\"    Cards played: {stats.cards_played}\")\n",
    "    print(f\"    Damage dealt: {stats.damage_dealt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed Comparison\n",
    "\n",
    "Compare decision-making speed of different agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_agent(factory, name, num_games=10):\n",
    "    \"\"\"Benchmark an agent's game speed.\"\"\"\n",
    "    opponent_factory = lambda pid: RandomPlayer(player_id=pid, seed=pid)\n",
    "\n",
    "    config = MatchConfig(\n",
    "        num_games=num_games,\n",
    "        player_factories=[factory, opponent_factory],\n",
    "        base_seed=42,\n",
    "    )\n",
    "\n",
    "    runner = MatchRunner()\n",
    "\n",
    "    start = time.time()\n",
    "    result = runner.run_match(config)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"games\": num_games,\n",
    "        \"total_time\": elapsed,\n",
    "        \"time_per_game\": elapsed / num_games,\n",
    "        \"win_rate\": result.win_rates.get(name.lower(), 0),\n",
    "    }\n",
    "\n",
    "\n",
    "# Benchmark each agent type\n",
    "agents_to_test = [\n",
    "    (lambda pid: RandomPlayer(player_id=pid, seed=pid), \"random\"),\n",
    "    (lambda pid: GreedyPlayer(player_id=pid), \"greedy\"),\n",
    "    (lambda pid: HeuristicPlayer(player_id=pid), \"heuristic\"),\n",
    "]\n",
    "\n",
    "print(\"Benchmarking agents (10 games each vs Random)...\\n\")\n",
    "results = []\n",
    "for factory, name in agents_to_test:\n",
    "    bench = benchmark_agent(factory, name)\n",
    "    results.append(bench)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Total time: {bench['total_time']:.2f}s\")\n",
    "    print(f\"  Per game: {bench['time_per_game'] * 1000:.1f}ms\")\n",
    "    print(f\"  Win rate vs Random: {bench['win_rate']:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS Configuration Tuning\n",
    "\n",
    "Explore how MCTS parameters affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different simulation counts\n",
    "simulation_counts = [10, 25, 50]\n",
    "mcts_results = []\n",
    "\n",
    "print(\"Testing MCTS with different simulation counts...\\n\")\n",
    "\n",
    "for num_sims in simulation_counts:\n",
    "    config = MCTSConfig(num_simulations=num_sims, max_rollout_depth=5)\n",
    "    factory = lambda pid, c=config: MCTSPlayer(player_id=pid, config=c, seed=pid)\n",
    "\n",
    "    bench = benchmark_agent(factory, \"mcts\", num_games=5)\n",
    "    bench[\"simulations\"] = num_sims\n",
    "    mcts_results.append(bench)\n",
    "\n",
    "    print(f\"MCTS (sims={num_sims}):\")\n",
    "    print(f\"  Per game: {bench['time_per_game'] * 1000:.1f}ms\")\n",
    "    print(f\"  Win rate vs Random: {bench['win_rate']:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head-to-Head Tournament\n",
    "\n",
    "Run a full round-robin tournament between all agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create factories for all agents\n",
    "all_factories = [\n",
    "    lambda pid: RandomPlayer(player_id=pid, seed=pid * 100),\n",
    "    lambda pid: GreedyPlayer(player_id=pid),\n",
    "    lambda pid: HeuristicPlayer(player_id=pid),\n",
    "]\n",
    "\n",
    "# Run round-robin\n",
    "print(\"Running round-robin tournament (20 games per matchup)...\\n\")\n",
    "\n",
    "config = BalanceConfig(games_per_matchup=20, base_seed=42)\n",
    "analyzer = BalanceAnalyzer(config)\n",
    "report = analyzer.run_analysis(all_factories, verbose=True)\n",
    "\n",
    "print(\"\\n\" + report.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matchup Matrix with Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tournament and get raw results\n",
    "runner = MatchRunner()\n",
    "raw_results = runner.run_round_robin(\n",
    "    player_factories=all_factories,\n",
    "    games_per_matchup=30,\n",
    "    base_seed=123,\n",
    ")\n",
    "\n",
    "# Create matchup matrix with statistical analysis\n",
    "matrix = create_matchup_matrix(raw_results)\n",
    "\n",
    "print(matrix.summary())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nRankings:\")\n",
    "for rank, (ptype, rate) in enumerate(matrix.get_rankings(), 1):\n",
    "    print(f\"  {rank}. {ptype}: {rate:.1%} average win rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Behavior Analysis\n",
    "\n",
    "Analyze decision patterns of different agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run games and collect detailed statistics\n",
    "def analyze_agent_behavior(factory, name, num_games=10):\n",
    "    \"\"\"Analyze an agent's behavior patterns.\"\"\"\n",
    "    opponent = lambda pid: RandomPlayer(player_id=pid, seed=pid + 500)\n",
    "\n",
    "    config = MatchConfig(\n",
    "        num_games=num_games,\n",
    "        player_factories=[factory, opponent],\n",
    "        base_seed=999,\n",
    "        log_events=False,\n",
    "    )\n",
    "\n",
    "    runner = MatchRunner()\n",
    "    result = runner.run_match(config)\n",
    "\n",
    "    # Aggregate stats from all games\n",
    "    total_bought = 0\n",
    "    total_played = 0\n",
    "    total_evolutions = 0\n",
    "\n",
    "    for game_result in result.results:\n",
    "        for pid, stats in game_result.player_stats.items():\n",
    "            if stats.player_type == name.lower():\n",
    "                total_bought += stats.cards_bought\n",
    "                total_played += stats.cards_played\n",
    "                total_evolutions += stats.evolutions\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"games\": num_games,\n",
    "        \"avg_cards_bought\": total_bought / num_games,\n",
    "        \"avg_cards_played\": total_played / num_games,\n",
    "        \"avg_evolutions\": total_evolutions / num_games,\n",
    "        \"win_rate\": result.win_rates.get(name.lower(), 0),\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze each agent\n",
    "print(\"Agent Behavior Analysis:\\n\")\n",
    "\n",
    "for factory, name in agents_to_test:\n",
    "    behavior = analyze_agent_behavior(factory, name, num_games=15)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Avg cards bought per game: {behavior['avg_cards_bought']:.1f}\")\n",
    "    print(f\"  Avg cards played per game: {behavior['avg_cards_played']:.1f}\")\n",
    "    print(f\"  Avg evolutions per game: {behavior['avg_evolutions']:.2f}\")\n",
    "    print(f\"  Win rate: {behavior['win_rate']:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Agent Strengths and Weaknesses\n",
    "\n",
    "| Agent | Strengths | Weaknesses |\n",
    "|-------|-----------|------------|\n",
    "| Random | Fast, unpredictable | No strategy |\n",
    "| Greedy | Good immediate value | No long-term planning |\n",
    "| Heuristic | Synergy building, evolution tracking | Fixed strategy |\n",
    "| MCTS | Adaptive, looks ahead | Slow, resource intensive |\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **For development testing**: Use Random or Greedy for fast iteration\n",
    "2. **For balance testing**: Use Heuristic as the baseline\n",
    "3. **For strong opponents**: Tune MCTS with appropriate simulation count\n",
    "4. **For analysis**: Compare agents across multiple seeds for statistical validity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
